{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to parse a workflow description and construct the task grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import itertools\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parses workflow description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_workflow(file_path):\n",
    "    \"\"\"Load workflow from a JSON file.\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Constructs a DAG of tasks based on dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_dag(workflow):\n",
    "    \"\"\"Construct a directed graph (DAG) representation of the workflow.\"\"\"\n",
    "    dag = {}\n",
    "    num_tasks = workflow[\"NumTasks\"]\n",
    "    \n",
    "    # Create nodes for all tasks\n",
    "    for i in range(1, num_tasks + 1):\n",
    "        task_name = f\"Task{i}\"\n",
    "        dag[task_name] = set()\n",
    "        # Add edge if this task has an input task\n",
    "        if \"InputTask\" in workflow[task_name]:\n",
    "            dag[workflow[task_name][\"InputTask\"]].add(task_name)\n",
    "    \n",
    "    print(f\"Constructed DAG: {dag}\")\n",
    "    return dag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Applies hard constraints to form initial maximal groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Util to decompose ScramArch into OS and CPU architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_os_and_arch(scram_arch):\n",
    "    # Split the ScramArch string into its components\n",
    "    os_part, arch, _ = scram_arch.split('_')\n",
    "    \n",
    "    # Extract just the numeric version from the OS part\n",
    "    # Strip all letters and keep only the number\n",
    "    os_version = ''.join(char for char in os_part if char.isdigit())\n",
    "    \n",
    "    return os_version, arch\n",
    "\n",
    "def parse_scram_arch_list(scram_arch_list):\n",
    "    results = []\n",
    "    for scram_arch in scram_arch_list:\n",
    "        os_version, arch = extract_os_and_arch(scram_arch)\n",
    "        results.append({\n",
    "            'os_version': os_version,\n",
    "            'cpu_arch': arch\n",
    "        })\n",
    "        print(f\"ScramArch: {scram_arch}, OS Version: {os_version}, CPU Architecture: {arch}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Evaluate hard constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_constraint(ti, tj):\n",
    "    \"\"\"\n",
    "    Check if two tasks satisfy hard constraints.\n",
    "    Args:\n",
    "        ti: First task (potential predecessor)\n",
    "        tj: Second task (potential successor)\n",
    "    Returns:\n",
    "        bool: True if tasks have same OS version and CPU architecture,\n",
    "              and tj either has no input task or depends on ti\n",
    "    \"\"\"\n",
    "    # Extract OS version and architecture for both tasks\n",
    "    ti_os, ti_arch = extract_os_and_arch(ti[\"ScramArch\"][0])  # Using [0] as ScramArch is a list\n",
    "    tj_os, tj_arch = extract_os_and_arch(tj[\"ScramArch\"][0])\n",
    "    \n",
    "    return (\n",
    "        ti_os == tj_os and\n",
    "        ti_arch == tj_arch and\n",
    "        (\"InputTask\" not in tj or tj[\"InputTask\"] == ti)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluates soft constraints to refine the grouping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_soft_constraint(ti, tj, weights):\n",
    "    \"\"\"Compute soft constraint compatibility score for two tasks.\"\"\"\n",
    "    score = (\n",
    "        weights[\"EvtThrou\"] * (1 / (ti[\"TimePerEvent\"] + tj[\"TimePerEvent\"])) +\n",
    "        weights[\"CpuEff\"] * (min(ti[\"Multicore\"], tj[\"Multicore\"]) / max(ti[\"Multicore\"], tj[\"Multicore\"])) +\n",
    "        weights[\"MemOcc\"] * (min(ti[\"Memory\"], tj[\"Memory\"]) / max(ti[\"Memory\"], tj[\"Memory\"])) +\n",
    "        weights[\"Acc\"] * (1 if ti[\"RequiresGPU\"] == tj[\"RequiresGPU\"] else 0)\n",
    "    )\n",
    "    return score / sum(weights.values())  # Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Iteratively updates the DAG and re-evaluates grouping decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_hard_groups(workflow):\n",
    "    \"\"\"Generate initial maximal groups using hard constraints.\"\"\"\n",
    "    tasks = [task for task in workflow if task.startswith(\"Task\")]\n",
    "    groups = []\n",
    "    \n",
    "    for task in tasks:\n",
    "        grouped = False\n",
    "        for group in groups:\n",
    "            if all(hard_constraint(workflow[task], workflow[t]) for t in group):\n",
    "                group.add(task)\n",
    "                grouped = True\n",
    "                break\n",
    "        if not grouped:\n",
    "            groups.append({task})\n",
    "    \n",
    "    return groups\n",
    "\n",
    "def refine_groups(groups, workflow, threshold, weights):\n",
    "    \"\"\"Refine groups using soft constraints.\"\"\"\n",
    "    refined_groups = []\n",
    "    \n",
    "    for group in groups:\n",
    "        group_tasks = list(group)\n",
    "        scores = np.zeros((len(group_tasks), len(group_tasks)))\n",
    "\n",
    "        # Compute pairwise soft constraints\n",
    "        for i, j in itertools.combinations(range(len(group_tasks)), 2):\n",
    "            scores[i, j] = scores[j, i] = compute_soft_constraint(\n",
    "                workflow[group_tasks[i]], workflow[group_tasks[j]], weights\n",
    "            )\n",
    "\n",
    "        # Compute average compatibility\n",
    "        print(f\"Scores: {scores}\")\n",
    "        print(f\"Group tasks: {group_tasks}\")\n",
    "        avg_score = np.sum(scores) / (len(group_tasks) * (len(group_tasks) - 1))\n",
    "\n",
    "        # Apply threshold\n",
    "        if avg_score < threshold:\n",
    "            best_split = max(\n",
    "                [(group_tasks[:i], group_tasks[i:]) for i in range(1, len(group_tasks))],\n",
    "                key=lambda g: min(compute_soft_constraint(workflow[g[0][0]], workflow[g[1][0]], weights), avg_score)\n",
    "            )\n",
    "            refined_groups.append(set(best_split[0]))\n",
    "            refined_groups.append(set(best_split[1]))\n",
    "        else:\n",
    "            refined_groups.append(group)\n",
    "\n",
    "    return refined_groups\n",
    "\n",
    "def regroup_and_iterate(workflow, threshold=0.5):\n",
    "    \"\"\"Iteratively evaluate DAG and refine groups.\"\"\"\n",
    "    weights = {\"EvtThrou\": 0.3, \"CpuEff\": 0.3, \"MemOcc\": 0.2, \"Acc\": 0.2}\n",
    "\n",
    "    while True:\n",
    "        groups = initial_hard_groups(workflow)\n",
    "        refined = refine_groups(groups, workflow, threshold, weights)\n",
    "\n",
    "        if refined == groups:\n",
    "            break  # Stop if no changes occur\n",
    "\n",
    "    return refined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Execute workflow composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [[0.    0.715]\n",
      " [0.715 0.   ]]\n",
      "Group tasks: ['Task2', 'Task1']\n",
      "Scores: [[0.]]\n",
      "Group tasks: ['Task3']\n",
      "Scores: [[0.]]\n",
      "Group tasks: ['Task4']\n",
      "[{'Task2', 'Task1'}, {'Task3'}, {'Task4'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ry/pr0j_gnj06j2ks3pm_44vq1c0000gq/T/ipykernel_80525/228503125.py:35: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  avg_score = np.sum(scores) / (len(group_tasks) * (len(group_tasks) - 1))\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "IN_FILE = \"../tests/ex0_perfect.json\"\n",
    "\n",
    "workflow = load_workflow(IN_FILE)\n",
    "groups = regroup_and_iterate(workflow)\n",
    "print(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
